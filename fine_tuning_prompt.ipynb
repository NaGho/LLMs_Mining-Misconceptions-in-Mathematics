{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-12-07T16:07:15.916995Z","iopub.status.busy":"2024-12-07T16:07:15.916112Z","iopub.status.idle":"2024-12-07T16:07:39.113825Z","shell.execute_reply":"2024-12-07T16:07:39.112856Z","shell.execute_reply.started":"2024-12-07T16:07:15.916940Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/kaggle-eedi/sample_submission.csv\n","/kaggle/input/kaggle-eedi/misconception_mapping.csv\n","/kaggle/input/kaggle-eedi/train.csv\n","/kaggle/input/kaggle-eedi/test.csv\n"]}],"source":["import numpy as np\n","# !pip install openai\n","# from openai import ChatCompletion\n","import pandas as pd\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn.functional as F\n","\n","from transformers import (\n","    AutoTokenizer, AutoModel, AutoModelForCausalLM, Trainer, TrainingArguments\n",")\n","import datasets\n","from tqdm import tqdm\n","import requests\n","import os\n","import re\n","\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-12-07T16:07:39.116190Z","iopub.status.busy":"2024-12-07T16:07:39.115601Z","iopub.status.idle":"2024-12-07T16:07:39.198579Z","shell.execute_reply":"2024-12-07T16:07:39.197617Z","shell.execute_reply.started":"2024-12-07T16:07:39.116156Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Internet access is available!\n"]}],"source":["try:\n","    response = requests.get(\"https://huggingface.co\", timeout=5)\n","    print(\"Internet access is available!\")\n","    ONLINE = True\n","except requests.exceptions.RequestException as e:\n","    print(f\"No internet access: {e}\")\n","    ONLINE = False\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-12-07T16:07:39.200093Z","iopub.status.busy":"2024-12-07T16:07:39.199762Z","iopub.status.idle":"2024-12-07T16:07:39.257666Z","shell.execute_reply":"2024-12-07T16:07:39.256501Z","shell.execute_reply.started":"2024-12-07T16:07:39.200062Z"},"trusted":true},"outputs":[],"source":["TRAIN = True\n","SAVE_PRETRAINED = True\n","# Load the datasets\n","input_folder = '/kaggle/input/'\n","\n","data_folder = input_folder + '/kaggle-eedi'# '/content/drive/MyDrive/eedi-mining-misconceptions-in-mathematics'\n","train_df = pd.read_csv(f'{data_folder}/train.csv')\n","test_df = pd.read_csv(f'{data_folder}/test.csv')\n","misconception_mapping = pd.read_csv(f'{data_folder}/misconception_mapping.csv')\n","# Create a dictionary to map MisconceptionId to its name\n","misconception_dict = dict(zip(\n","    misconception_mapping[\"MisconceptionId\"], \n","    misconception_mapping[\"MisconceptionName\"]\n","))"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-12-07T16:07:39.260542Z","iopub.status.busy":"2024-12-07T16:07:39.260232Z","iopub.status.idle":"2024-12-07T16:07:39.353910Z","shell.execute_reply":"2024-12-07T16:07:39.352924Z","shell.execute_reply.started":"2024-12-07T16:07:39.260511Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1869/1869 [00:00<00:00, 47355.05it/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1869/1869 [00:00<00:00, 54941.05it/s]\n"]}],"source":["# All Known Misconceptions:\n","#     {misconception_dict}\n","        \n","# Function to create a prompt for a single question\n","def generate_input_prompt(row):\n","    question_text = row[\"QuestionText\"]\n","    options = [\n","        f\"A: {row['AnswerAText']}\",\n","        f\"B: {row['AnswerBText']}\",\n","        f\"C: {row['AnswerCText']}\",\n","        f\"D: {row['AnswerDText']} (Correct)\"\n","    ]\n","    # Match misconceptions to distractors in a multiple-choice Diagnostic Question.\n","    prompt = f\"\"\"\n","        Question:\n","        {question_text}\n","        \n","        Options:\n","        {options[0]}\n","        {options[1]}\n","        {options[2]}\n","        {options[3]}\n","        \n","        Correct Answer:\n","        {row['CorrectAnswer']}\n","        \n","        Task: Predict the most likely misconception IDs for each distractor (B, C, D) and rank them by probability.\n","        \n","        Output format:\n","        A - MisconceptionId - MisconceptionName, if it is not the correct answer\n","        B - MisconceptionId - MisconceptionName, if it is not the correct answer\n","        C - MisconceptionId - MisconceptionName, if it is not the correct answer\n","        D - MisconceptionId - MisconceptionName, if it is not the correct answer\n","    \"\"\"\n","    return prompt\n","\n","\n","def generate_output_prompt(row):\n","    prompt = \"\"\"\"\"\"\n","    for ans in list(set(['A', 'B', 'C', 'D'])-set(row['CorrectAnswer'])):\n","        misconc_id = row[f'Misconception{ans}Id']\n","        # print(\"misconc_id = \", misconc_id)\n","        try:\n","            prompt += f\"\"\"{ans} - {misconc_id} - {misconception_dict[misconc_id]}\n","                \"\"\"\n","        except:\n","            pass\n","        # else:\n","        #     prompt += f\"\"\"{ans} - Unavailable\n","        #         \"\"\"\n","        \n","    return prompt\n","\n","tqdm.pandas()\n","train_df['input'] = train_df.progress_apply(generate_input_prompt, axis=1)\n","train_df['output'] = train_df.progress_apply(generate_output_prompt, axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-07T16:07:39.355294Z","iopub.status.busy":"2024-12-07T16:07:39.355014Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aae6fa57911a48cc81ce71540c268136","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b874cdef7d354e36852615393299a08c","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bc589eef5ccc478cbcd6c07feb9ed017","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"35f4844373e049b0b9a43af2033bc387","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d56031f1156548db91d02e924c795edc","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c6db66861ac46bf97516479f095c782","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1869 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af55ba86074a435c8d3b302591c26078","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f759f91054cf42fbb51f5b4c76175e7d","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e69038ce06004fb084d1b1d24d5cf818","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/tmp/ipykernel_23/1873072431.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n"]}],"source":["model_name = \"EleutherAI/gpt-neo-125M\"\n","# Load and tokenize the dataset\n","# train_data = datasets.load_dataset(\"csv\", data_files=f'{data_folder}/train.csv')[\"train\"]\n","train_data = datasets.Dataset.from_pandas(train_df)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","# Assign the existing `eos_token` as the `pad_token`\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","\n","def preprocess_function(examples):\n","    return tokenizer(\n","        examples[\"input\"], \n","        text_target=examples[\"output\"], \n","        max_length=512, \n","        padding=\"max_length\",\n","        truncation=True\n","    )\n","\n","tokenized_data = train_data.map(preprocess_function, batched=True)\n","\n","# Load the model\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"no\",\n","    learning_rate=5e-5,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    logging_dir=\"./logs\",\n","    save_total_limit=1,\n","    per_device_train_batch_size=8,\n",")\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_data,\n","    tokenizer=tokenizer\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Save the fine-tuned model\n","trainer.save_model(\"./fine_tuned_model\")\n","tokenizer.save_pretrained(\"./fine_tuned_model\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # Function to predict misconceptions using the OpenAI API\n","# def predict_misconceptions(test_df, misconception_dict):\n","#     predictions = []\n","    \n","#     for _, row in test_df.iterrows():\n","#         prompt = generate_prompt(row, misconception_dict)\n","        \n","#         try:\n","#             # Call OpenAI API\n","#             response = ChatCompletion.create(\n","#                 model=\"gpt-4\",\n","#                 messages=[{\"role\": \"user\", \"content\": prompt}]\n","#             )\n","#             output = response[\"choices\"][0][\"message\"][\"content\"].strip()\n","#             predictions.append((row[\"QuestionId\"], output))\n","        \n","#         except Exception as e:\n","#             print(f\"Error processing QuestionId {row['QuestionId']}: {e}\")\n","#             predictions.append((row[\"QuestionId\"], \"Error\"))\n","    \n","#     return predictions\n","\n","# # Save predictions to the submission format\n","# def save_submission(predictions, filename=\"submission.csv\"):\n","#     submission_df = pd.DataFrame(predictions, columns=[\"QuestionId_Answer\", \"MisconceptionId\"])\n","#     submission_df.to_csv(filename, index=False)\n","#     print(f\"Submission saved to {filename}\")\n","\n","# # Main logic\n","# # if __name__ == \"__main__\":\n","# # Generate predictions\n","# predictions = predict_misconceptions(test_df, misconception_dict)\n","\n","# # Save to submission file\n","# save_submission(predictions)\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":6237503,"sourceId":10110674,"sourceType":"datasetVersion"}],"dockerImageVersionId":30805,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6 (main, Sep  6 2024, 19:03:47) [Clang 15.0.0 (clang-1500.1.0.2.5)]"},"vscode":{"interpreter":{"hash":"82dd3684a8135ffe99fa19764570bbe52ec0c13ae1a8f5c4c6b44fada8b85f3b"}}},"nbformat":4,"nbformat_minor":4}
