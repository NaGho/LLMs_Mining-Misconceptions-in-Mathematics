{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-11-27T05:05:24.279546Z","iopub.status.busy":"2024-11-27T05:05:24.279271Z","iopub.status.idle":"2024-11-27T05:05:41.739311Z","shell.execute_reply":"2024-11-27T05:05:41.738653Z","shell.execute_reply.started":"2024-11-27T05:05:24.279520Z"},"executionInfo":{"elapsed":59713,"status":"ok","timestamp":1728589878739,"user":{"displayName":"Nafiseh Gh","userId":"08834597136233274048"},"user_tz":240},"id":"p-U8EVB6GNw3","outputId":"9de55e78-b46b-40d2-fd8a-4b9d3c3d6ca1","trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn.functional as F\n","\n","from transformers import (\n","    GPT2Tokenizer, GPT2Model, BertTokenizer, BertModel,TrainingArguments,\n","    AutoTokenizer, AutoModelForSequenceClassification, Trainer,\n","    DistilBertTokenizer, DistilBertModel\n",")\n","# from pycaret.classification import *\n","# !pip install -U sentence-transformers\n","# from sentence_transformers import SentenceTransformer, InputExample, losses\n","# !pip install datasets==2.6.1\n","# from datasets import Dataset\n","from tqdm import tqdm\n","import requests\n","import os\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-11-27T05:05:41.741865Z","iopub.status.busy":"2024-11-27T05:05:41.740965Z","iopub.status.idle":"2024-11-27T05:05:41.815955Z","shell.execute_reply":"2024-11-27T05:05:41.815263Z","shell.execute_reply.started":"2024-11-27T05:05:41.741826Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Internet access is available!\n"]}],"source":["try:\n","    response = requests.get(\"https://huggingface.co\", timeout=5)\n","    print(\"Internet access is available!\")\n","    ONLINE = True\n","except requests.exceptions.RequestException as e:\n","    print(f\"No internet access: {e}\")\n","    ONLINE = False\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-11-27T05:05:41.817028Z","iopub.status.busy":"2024-11-27T05:05:41.816783Z","iopub.status.idle":"2024-11-27T05:05:41.866312Z","shell.execute_reply":"2024-11-27T05:05:41.865726Z","shell.execute_reply.started":"2024-11-27T05:05:41.817003Z"},"executionInfo":{"elapsed":1073,"status":"ok","timestamp":1728589949501,"user":{"displayName":"Nafiseh Gh","userId":"08834597136233274048"},"user_tz":240},"id":"ciYZc3r6GNw5","trusted":true},"outputs":[],"source":["TRAIN = True\n","# Load the datasets\n","data_folder = '/kaggle/input/eedi-mining-misconceptions-in-mathematics'# '/content/drive/MyDrive/eedi-mining-misconceptions-in-mathematics'\n","train_df = pd.read_csv(f'{data_folder}/train.csv')\n","test_df = pd.read_csv(f'{data_folder}/test.csv')\n","indicator_mapping = pd.read_csv(f'{data_folder}/misconception_mapping.csv')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-11-27T05:05:41.867969Z","iopub.status.busy":"2024-11-27T05:05:41.867731Z","iopub.status.idle":"2024-11-27T05:05:41.875117Z","shell.execute_reply":"2024-11-27T05:05:41.874203Z","shell.execute_reply.started":"2024-11-27T05:05:41.867945Z"},"executionInfo":{"elapsed":143,"status":"ok","timestamp":1728593851886,"user":{"displayName":"Nafiseh Gh","userId":"08834597136233274048"},"user_tz":240},"id":"3m2TeK2O1Oc7","trusted":true},"outputs":[],"source":["def preprocess_dataframe(df, misconception=False):\n","  id_vars = [\"QuestionText\", 'QuestionId']\n","  var_name = 'Answer'\n","  answer_melted = df.melt(\n","      id_vars=id_vars,\n","      value_vars=[f'Answer{let}Text' for let in ['A', 'B', 'C', 'D']],\n","      value_name='AnswerText',\n","      var_name=var_name\n","  )\n","  for strr in ['Answer', 'Text']:\n","    answer_melted[var_name] = answer_melted[var_name].str.replace(strr, '')\n","\n","  if misconception:\n","    misconception_melted = df.melt(\n","        id_vars=id_vars, value_vars=[f'Misconception{let}Id' for let in ['A', 'B', 'C', 'D']],\n","        value_name='MisconceptionId', var_name=var_name\n","    )\n","    for strr in ['Misconception', 'Id']:\n","      misconception_melted[var_name] = misconception_melted[var_name].str.replace(strr, '')\n","\n","    df = pd.merge(answer_melted, misconception_melted, on=['QuestionText', var_name])\n","  else:\n","    df = answer_melted\n","  df['Q&A'] = df['QuestionText'] + ' ' + df['AnswerText']\n","\n","  if misconception:\n","    df = pd.merge(df, indicator_mapping, on='MisconceptionId')\n","\n","  df['label'] = 1 # 1 if the texts are related, 0 otherwise\n","  df = df.dropna(subset=['Q&A'])\n","\n","  return df"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-11-27T05:05:41.876800Z","iopub.status.busy":"2024-11-27T05:05:41.876359Z","iopub.status.idle":"2024-11-27T05:07:21.311741Z","shell.execute_reply":"2024-11-27T05:07:21.310825Z","shell.execute_reply.started":"2024-11-27T05:05:41.876763Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4cd7ad287d1248f7af05c29920daf3d4","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff2954a165a641318aa306de897d62df","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0a46eda78f7474f9b44f8f583f65416","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26726ed27c7b43efb2dab653542ba10e","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["data loader is ready\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"92d915152dc64a44b41fddf6dd4b584b","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["epoch 0 ...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 279/279 [01:36<00:00,  2.89it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.01280074413969285\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Define Dataset\n","class TextPairDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len=128):\n","        self.qa_texts = dataframe['Q&A'].tolist()\n","        self.misconception_texts = dataframe['MisconceptionName'].tolist()\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.qa_texts)\n","\n","    def __getitem__(self, idx):\n","        # Handle both single and batch indices\n","        if isinstance(idx, int):\n","            idx = [idx]  # Convert single index to list for uniform processing\n","    \n","        # Process batch\n","        qa_texts = [self.qa_texts[i] for i in idx]\n","        misconception_texts = [self.misconception_texts[i] for i in idx]\n","    \n","        # Tokenize batch of texts\n","        qa_encodings = self.tokenizer(qa_texts, max_length=self.max_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n","        misconception_encodings = self.tokenizer(misconception_texts, max_length=self.max_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n","    \n","        # Return as a dictionary to avoid BatchEncoding issues\n","        return {\n","            \"qa_input_ids\": qa_encodings[\"input_ids\"],\n","            \"qa_attention_mask\": qa_encodings[\"attention_mask\"],\n","            \"misconception_input_ids\": misconception_encodings[\"input_ids\"],\n","            \"misconception_attention_mask\": misconception_encodings[\"attention_mask\"],\n","        }\n","\n","# Define Contrastive Model\n","class ContrastiveLearningModel(nn.Module):\n","    def __init__(self, model_name):\n","        super(ContrastiveLearningModel, self).__init__()\n","        self.bert_model = DistilBertModel.from_pretrained(model_name)\n","        self.fc = nn.Linear(self.bert_model.config.hidden_size, 128)  # Project to a smaller dimension\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n","        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n","        return self.fc(cls_output)\n","\n","# Define Contrastive Loss\n","class ContrastiveLoss(nn.Module):\n","    def __init__(self, margin=1.0):\n","        super(ContrastiveLoss, self).__init__()\n","        self.margin = margin\n","\n","    def forward(self, embedding_a, embedding_b, label):\n","        # Label: 1 for similar, 0 for dissimilar\n","        cosine_similarity = nn.functional.cosine_similarity(embedding_a, embedding_b)\n","        loss = label * (1 - cosine_similarity) + (1 - label) * torch.clamp(cosine_similarity - self.margin, min=0)\n","        return loss.mean()\n","\n","df = preprocess_dataframe(train_df, misconception=True)\n","\n","# dataset_train = Dataset.from_pandas(df[:len(df)-200])\n","# dataset_eval = Dataset.from_pandas(df[len(df)-200:])\n","pre_trained_model_name = \"distilbert-base-uncased\"\n","local_pre_trained_name = f\"./{pre_trained_model_name}\"\n","\n","# tokenizer = AutoTokenizer.from_pretrained(pre_trained_model_name)\n","tokenizer = DistilBertTokenizer.from_pretrained(pre_trained_model_name)\n","\n","# if SAVE_PRETRAINED:\n","#     tokenizer.save_pretrained(local_pre_trained_name)\n","#     model.save_pretrained(local_pre_trained_name)\n","        \n","# Prepare DataLoader\n","# tokenized_dataset_train = dataset_train.map(tokenize_function, batched=True)\n","# tokenized_dataset_eval = dataset_eval.map(tokenize_function, batched=True)\n","dataset = TextPairDataset(df, tokenizer)\n","data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n","print('data loader is ready')\n","\n","# Initialize Model, Loss, Optimizer\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = ContrastiveLearningModel(pre_trained_model_name).to(device)\n","loss_fn = ContrastiveLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n","\n","# Training Loop\n","def train_contrastive_model(data_loader, model, loss_fn, optimizer, device):\n","    model.train()\n","    total_loss = 0\n","    for batch in tqdm(data_loader):\n","        qa_input_ids = batch[\"qa_input_ids\"].squeeze(1).to(device)\n","        qa_attention_mask = batch[\"qa_attention_mask\"].squeeze(1).to(device)\n","        misconception_input_ids = batch[\"misconception_input_ids\"].squeeze(1).to(device)\n","        misconception_attention_mask = batch[\"misconception_attention_mask\"].squeeze(1).to(device)\n","        \n","        # # qa_encodings, misconception_encodings = batch\n","        # input_ids_qa = qa_encodings['input_ids'].squeeze(1).to(device)\n","        # attention_mask_qa = qa_encodings['attention_mask'].squeeze(1).to(device)\n","        # input_ids_mis = misconception_encodings['input_ids'].squeeze(1).to(device)\n","        # attention_mask_mis = misconception_encodings['attention_mask'].squeeze(1).to(device)\n","\n","        # Forward pass\n","        embeddings_qa = model(qa_input_ids, qa_attention_mask)\n","        embeddings_mis = model(misconception_input_ids, misconception_attention_mask)\n","        # Create labels: 1 for matched pairs\n","        labels = torch.ones(embeddings_qa.size(0)).to(device)\n","        loss = loss_fn(embeddings_qa, embeddings_mis, labels)\n","\n","        # Backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","    return total_loss / len(data_loader)\n","\n","if TRAIN:\n","    # Train Model\n","    for epoch in range(1):  # Run for 5 epochs\n","        print(f'epoch {epoch} ...')\n","        avg_loss = train_contrastive_model(data_loader, model, loss_fn, optimizer, device)\n","        print(f\"Epoch {epoch + 1}, Loss: {avg_loss}\")\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-11-27T05:08:40.885454Z","iopub.status.busy":"2024-11-27T05:08:40.885120Z","iopub.status.idle":"2024-11-27T05:08:41.574308Z","shell.execute_reply":"2024-11-27T05:08:41.573247Z","shell.execute_reply.started":"2024-11-27T05:08:40.885427Z"},"trusted":true},"outputs":[],"source":["if ONLINE:\n","    # Save tokenizer and model\n","    output_dir = \"./fine_tuned_distilbert\"\n","    tokenizer.save_pretrained(\"./distilbert_tokenizer\")\n","    # Save the model's state_dict\n","    torch.save(model.state_dict(), './contrastive_learning_model.pth')\n","\n","    # model.save_pretrained(output_dir)\n","    # # Load the fine-tuned model and tokenizer\n","    # tokenizer = AutoTokenizer.from_pretrained(local_pre_trained_name, local_files_only=True)\n","    # model_name = \"/kaggle/working/fine_tuned_model\"  # Path to your fine-tuned model\n","    # model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","else:\n","    # Recreate the model instance\n","    model = ContrastiveLearningModel(pre_trained_model_name).to(device)\n","    \n","    # Load the saved state_dict into the model\n","    model.load_state_dict(torch.load('/kaggle/input/my_fine_tuned/transformers/default/1/contrastive_learning_model.pth'))\n","    \n","    # Set the model to evaluation mode (optional, if you're in inference mode)\n","    model.eval()\n","\n","    # model_name = \"/kaggle/input/my_fine_tuned/transformers/default/1/fine_tuned_distilbert\"  # Path to your fine-tuned model\n","    # tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","    # model = ContrastiveLearningModel(model_name).to(device)\n","\n","    # model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","    # tokenizer_path = \"/kaggle/input/my_distilbert_model/pytorch/default/1/distilbert-base-uncased\"\n","    # tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n","\n","    "]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-11-27T05:08:57.056375Z","iopub.status.busy":"2024-11-27T05:08:57.056043Z","iopub.status.idle":"2024-11-27T05:09:15.374157Z","shell.execute_reply":"2024-11-27T05:09:15.373320Z","shell.execute_reply.started":"2024-11-27T05:08:57.056350Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2587/2587 [00:18<00:00, 141.33it/s]\n"]}],"source":["# Compute similarity with all misconception texts\n","misconception_embeddings = []\n","misconception_texts = indicator_mapping['MisconceptionName'].tolist()\n","miconception_ids = indicator_mapping['MisconceptionId'].tolist()\n","\n","for text in tqdm(misconception_texts):\n","    encodings = tokenizer(text, max_length=128, padding='max_length', truncation=True, return_tensors=\"pt\")\n","    input_ids = encodings['input_ids'].to(device)\n","    attention_mask = encodings['attention_mask'].to(device)\n","    misconception_embeddings.append(model(input_ids, attention_mask).detach())\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-11-27T05:09:15.376737Z","iopub.status.busy":"2024-11-27T05:09:15.375955Z","iopub.status.idle":"2024-11-27T05:09:15.653707Z","shell.execute_reply":"2024-11-27T05:09:15.652906Z","shell.execute_reply.started":"2024-11-27T05:09:15.376709Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Top Matches: [114, 2136, 2409, 261, 791, 2182, 793, 329, 2106, 2475, 930, 388, 1544, 1619, 955, 2284, 1432, 2187, 824, 2406, 2162, 285, 1765, 2112, 2130]\n"]}],"source":["# Inference: Find Top 25 Matches\n","def find_top_matches(test_qa, model, tokenizer, device, top_k=25):\n","    # Tokenize and encode the test Q&A\n","    test_encodings = tokenizer(test_qa, max_length=128, padding='max_length', truncation=True, return_tensors=\"pt\")\n","    input_ids = test_encodings['input_ids'].to(device)\n","    attention_mask = test_encodings['attention_mask'].to(device)\n","    \n","    test_embedding = model(input_ids, attention_mask).detach()\n","\n","    \n","    similarities = [nn.functional.cosine_similarity(test_embedding, embedding, dim=1).item() for embedding in misconception_embeddings]\n","    sorted_indices = torch.argsort(torch.tensor(similarities), descending=True)\n","    \n","    return [miconception_ids[i] for i in sorted_indices[:top_k]]\n","\n","\n","# Test with a new Q&A\n","test_qa = \"Sample question and answer text here.\"\n","top_matches = find_top_matches(test_qa, model, tokenizer, device)\n","print(\"Top Matches:\", top_matches)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-11-27T05:20:49.080036Z","iopub.status.busy":"2024-11-27T05:20:49.079387Z","iopub.status.idle":"2024-11-27T05:20:52.381346Z","shell.execute_reply":"2024-11-27T05:20:52.380724Z","shell.execute_reply.started":"2024-11-27T05:20:49.080002Z"},"executionInfo":{"elapsed":3513,"status":"ok","timestamp":1728593857812,"user":{"displayName":"Nafiseh Gh","userId":"08834597136233274048"},"user_tz":240},"id":"Cl0JvDYNv5_J","outputId":"94a234ea-2d68-4d65-e480-71a0cb60ae78","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 12/12 [00:03<00:00,  3.67it/s]\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>QuestionText</th>\n","      <th>QuestionId</th>\n","      <th>Answer</th>\n","      <th>AnswerText</th>\n","      <th>Q&amp;A</th>\n","      <th>label</th>\n","      <th>MisconceptionId</th>\n","      <th>QuestionId_Answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n","      <td>1869</td>\n","      <td>A</td>\n","      <td>\\( 3 \\times(2+4)-5 \\)</td>\n","      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n","      <td>1</td>\n","      <td>[94, 303, 839, 429, 937, 480, 994, 519, 1342, ...</td>\n","      <td>1869_A</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Simplify the following, if possible: \\( \\frac{...</td>\n","      <td>1870</td>\n","      <td>A</td>\n","      <td>\\( m+1 \\)</td>\n","      <td>Simplify the following, if possible: \\( \\frac{...</td>\n","      <td>1</td>\n","      <td>[429, 2469, 925, 1360, 811, 2116, 272, 2139, 3...</td>\n","      <td>1870_A</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n","      <td>1871</td>\n","      <td>A</td>\n","      <td>Only\\nTom</td>\n","      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n","      <td>1</td>\n","      <td>[584, 480, 566, 1413, 2137, 2564, 2251, 2293, ...</td>\n","      <td>1871_A</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n","      <td>1869</td>\n","      <td>B</td>\n","      <td>\\( 3 \\times 2+(4-5) \\)</td>\n","      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n","      <td>1</td>\n","      <td>[429, 2475, 1042, 265, 1759, 1982, 2000, 2221,...</td>\n","      <td>1869_A</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Simplify the following, if possible: \\( \\frac{...</td>\n","      <td>1870</td>\n","      <td>B</td>\n","      <td>\\( m+2 \\)</td>\n","      <td>Simplify the following, if possible: \\( \\frac{...</td>\n","      <td>1</td>\n","      <td>[429, 620, 2174, 1239, 2400, 925, 887, 363, 20...</td>\n","      <td>1870_A</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n","      <td>1871</td>\n","      <td>B</td>\n","      <td>Only\\nKatie</td>\n","      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n","      <td>1</td>\n","      <td>[2096, 908, 57, 2257, 2385, 1196, 475, 179, 21...</td>\n","      <td>1871_A</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n","      <td>1869</td>\n","      <td>C</td>\n","      <td>\\( 3 \\times(2+4-5) \\)</td>\n","      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n","      <td>1</td>\n","      <td>[736, 429, 2446, 1342, 303, 546, 2335, 711, 16...</td>\n","      <td>1869_A</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Simplify the following, if possible: \\( \\frac{...</td>\n","      <td>1870</td>\n","      <td>C</td>\n","      <td>\\( m-1 \\)</td>\n","      <td>Simplify the following, if possible: \\( \\frac{...</td>\n","      <td>1</td>\n","      <td>[429, 329, 1432, 1991, 578, 1821, 2054, 405, 8...</td>\n","      <td>1870_A</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n","      <td>1871</td>\n","      <td>C</td>\n","      <td>Both Tom and Katie</td>\n","      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n","      <td>1</td>\n","      <td>[272, 937, 887, 2068, 1536, 2372, 2096, 2385, ...</td>\n","      <td>1871_A</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n","      <td>1869</td>\n","      <td>D</td>\n","      <td>Does not need brackets</td>\n","      <td>\\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...</td>\n","      <td>1</td>\n","      <td>[1110, 1994, 285, 2068, 1817, 889, 365, 1042, ...</td>\n","      <td>1869_A</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Simplify the following, if possible: \\( \\frac{...</td>\n","      <td>1870</td>\n","      <td>D</td>\n","      <td>Does not simplify</td>\n","      <td>Simplify the following, if possible: \\( \\frac{...</td>\n","      <td>1</td>\n","      <td>[429, 78, 786, 2008, 1778, 1323, 1114, 2518, 9...</td>\n","      <td>1870_A</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n","      <td>1871</td>\n","      <td>D</td>\n","      <td>Neither is correct</td>\n","      <td>Tom and Katie are discussing the \\( 5 \\) plant...</td>\n","      <td>1</td>\n","      <td>[155, 608, 961, 887, 1598, 1413, 356, 703, 241...</td>\n","      <td>1871_A</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                         QuestionText  QuestionId Answer  \\\n","0   \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...        1869      A   \n","1   Simplify the following, if possible: \\( \\frac{...        1870      A   \n","2   Tom and Katie are discussing the \\( 5 \\) plant...        1871      A   \n","3   \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...        1869      B   \n","4   Simplify the following, if possible: \\( \\frac{...        1870      B   \n","5   Tom and Katie are discussing the \\( 5 \\) plant...        1871      B   \n","6   \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...        1869      C   \n","7   Simplify the following, if possible: \\( \\frac{...        1870      C   \n","8   Tom and Katie are discussing the \\( 5 \\) plant...        1871      C   \n","9   \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...        1869      D   \n","10  Simplify the following, if possible: \\( \\frac{...        1870      D   \n","11  Tom and Katie are discussing the \\( 5 \\) plant...        1871      D   \n","\n","                AnswerText                                                Q&A  \\\n","0    \\( 3 \\times(2+4)-5 \\)  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...   \n","1                \\( m+1 \\)  Simplify the following, if possible: \\( \\frac{...   \n","2                Only\\nTom  Tom and Katie are discussing the \\( 5 \\) plant...   \n","3   \\( 3 \\times 2+(4-5) \\)  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...   \n","4                \\( m+2 \\)  Simplify the following, if possible: \\( \\frac{...   \n","5              Only\\nKatie  Tom and Katie are discussing the \\( 5 \\) plant...   \n","6    \\( 3 \\times(2+4-5) \\)  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...   \n","7                \\( m-1 \\)  Simplify the following, if possible: \\( \\frac{...   \n","8       Both Tom and Katie  Tom and Katie are discussing the \\( 5 \\) plant...   \n","9   Does not need brackets  \\[\\n3 \\times 2+4-5\\n\\]\\nWhere do the brackets ...   \n","10       Does not simplify  Simplify the following, if possible: \\( \\frac{...   \n","11      Neither is correct  Tom and Katie are discussing the \\( 5 \\) plant...   \n","\n","    label                                    MisconceptionId QuestionId_Answer  \n","0       1  [94, 303, 839, 429, 937, 480, 994, 519, 1342, ...            1869_A  \n","1       1  [429, 2469, 925, 1360, 811, 2116, 272, 2139, 3...            1870_A  \n","2       1  [584, 480, 566, 1413, 2137, 2564, 2251, 2293, ...            1871_A  \n","3       1  [429, 2475, 1042, 265, 1759, 1982, 2000, 2221,...            1869_A  \n","4       1  [429, 620, 2174, 1239, 2400, 925, 887, 363, 20...            1870_A  \n","5       1  [2096, 908, 57, 2257, 2385, 1196, 475, 179, 21...            1871_A  \n","6       1  [736, 429, 2446, 1342, 303, 546, 2335, 711, 16...            1869_A  \n","7       1  [429, 329, 1432, 1991, 578, 1821, 2054, 405, 8...            1870_A  \n","8       1  [272, 937, 887, 2068, 1536, 2372, 2096, 2385, ...            1871_A  \n","9       1  [1110, 1994, 285, 2068, 1817, 889, 365, 1042, ...            1869_A  \n","10      1  [429, 78, 786, 2008, 1778, 1323, 1114, 2518, 9...            1870_A  \n","11      1  [155, 608, 961, 887, 1598, 1413, 356, 703, 241...            1871_A  "]},"metadata":{},"output_type":"display_data"}],"source":["# Inference loop\n","# prepare test data\n","df_test_prep = preprocess_dataframe(test_df, misconception=False)\n","\n","tqdm.pandas()\n","# Append the predictions to the original DataFrame\n","df_test_prep['MisconceptionId'] = df_test_prep.progress_apply(\n","    lambda x: list(find_top_matches(x['Q&A'], model, tokenizer, device)), axis=1\n",")\n","\n","# Print the DataFrame with predictions\n","# display(df_test_prep)\n","\n","df_test_prep['QuestionId_Answer'] = df_test_prep['QuestionId'].astype(str) + '_' + df['Answer']\n","# df_test_prep = df_test_prep.sort_values(by=['QuestionId_Answer', 'prediction'], ascending=[True, False])\n","# df_out = df_test_prep.groupby(['QuestionId_Answer', 'Answer']).apply(\n","#     lambda x: x['MisconceptionId'].head(25).tolist()\n","# ).reset_index().rename(columns={0: 'MisconceptionId'})\n","df_out = df_test_prep\n","display(df_out)\n","df_out[['QuestionId_Answer', 'MisconceptionId']].to_csv('./v11_submission.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-11-27T05:07:44.989707Z","iopub.status.idle":"2024-11-27T05:07:44.990220Z","shell.execute_reply":"2024-11-27T05:07:44.989999Z","shell.execute_reply.started":"2024-11-27T05:07:44.989976Z"},"trusted":true},"outputs":[],"source":["print(os.listdir(\"/kaggle/input/\"))\n","print(os.listdir(\"/kaggle/input/my_distilbert_model/pytorch/default/1/distilbert-base-uncased\"))"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/NaGho/Kaggle_Eedi/blob/main/main.ipynb","timestamp":1727216025251}],"toc_visible":true},"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":9738540,"sourceId":82695,"sourceType":"competition"},{"modelId":136376,"modelInstanceId":113044,"sourceId":133696,"sourceType":"modelInstanceVersion"},{"modelId":167859,"modelInstanceId":145303,"sourceId":170763,"sourceType":"modelInstanceVersion"},{"modelId":175888,"modelInstanceId":153416,"sourceId":180067,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6 (main, Sep  6 2024, 19:03:47) [Clang 15.0.0 (clang-1500.1.0.2.5)]"},"vscode":{"interpreter":{"hash":"82dd3684a8135ffe99fa19764570bbe52ec0c13ae1a8f5c4c6b44fada8b85f3b"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"07de38611fb946ff9bf967c9c771f528":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"12e6d489cd5b46c5beb989cfb0b160aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_819e33ba232b46b4adba95fde8ec0a49","IPY_MODEL_32aaa46f841249e68f2145f108240bd8","IPY_MODEL_ef14dd528b0848d69e5c12c611cf4fa3"],"layout":"IPY_MODEL_361d881113dd4ae18eba794f4347e3db"}},"196a8763bb44474a91b8feaf6fce0c62":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20cfd2b9f94e40849fc4f274024c6436":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ca023fed5784ff0b34205241dd85ffe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32aaa46f841249e68f2145f108240bd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_9af83cc00f00480b83d9b1afb8ad881c","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_445881c7b15f427cbc40b652c53987a2","value":4}},"361d881113dd4ae18eba794f4347e3db":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"445881c7b15f427cbc40b652c53987a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5098d61608e3419a9122641c32434e6e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_592069db21ce46f99dc91a64004260d3","placeholder":"​","style":"IPY_MODEL_7dad968eb7ef4501b1bea62385ec932c","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"592069db21ce46f99dc91a64004260d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bb2e88908c94dc5a7b8408832258ecf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c82e3226baf24831b726b4e6feb87aca","IPY_MODEL_9eacb068d89b4498a47bc6abdf9442e1","IPY_MODEL_5098d61608e3419a9122641c32434e6e"],"layout":"IPY_MODEL_20cfd2b9f94e40849fc4f274024c6436"}},"6783a04660f9450cbba86b71d4257a43":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7dad968eb7ef4501b1bea62385ec932c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"819e33ba232b46b4adba95fde8ec0a49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_faffb72a02954e69ba62415a8423a180","placeholder":"​","style":"IPY_MODEL_8369e98de50f4b94b6803c55e719ccd4","value":" 80%"}},"8369e98de50f4b94b6803c55e719ccd4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9536f7654dc248d48406316e44fcf2ad":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9af83cc00f00480b83d9b1afb8ad881c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9eacb068d89b4498a47bc6abdf9442e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ca023fed5784ff0b34205241dd85ffe","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6783a04660f9450cbba86b71d4257a43","value":0}},"c82e3226baf24831b726b4e6feb87aca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9536f7654dc248d48406316e44fcf2ad","placeholder":"​","style":"IPY_MODEL_ca3dbe76639b4326af244fb856c1af2e","value":"  0%"}},"ca3dbe76639b4326af244fb856c1af2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef14dd528b0848d69e5c12c611cf4fa3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_196a8763bb44474a91b8feaf6fce0c62","placeholder":"​","style":"IPY_MODEL_07de38611fb946ff9bf967c9c771f528","value":" 4/5 [00:02&lt;00:00,  1.74ba/s]"}},"faffb72a02954e69ba62415a8423a180":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":4}
